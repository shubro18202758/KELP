"""
Vision-Language Model Engine
Uses Qwen3-VL-8B-Instruct with GPU optimization for:
1. Understanding web-scraped images/charts
2. Generating unique layout blueprints
3. Smart content extraction from visual data
"""
import torch
import gc
import json
import random
import hashlib
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from datetime import datetime
import base64
import io
import sys

sys.path.insert(0, str(Path(__file__).parent.parent.parent))


@dataclass
class VLMConfig:
    """Configuration for Vision-Language Model"""
    model_name: str = "Qwen/Qwen2.5-VL-7B-Instruct"  # VL model
    device: str = "cuda"
    torch_dtype: str = "bfloat16"  # Best for RTX 4070
    use_flash_attention: bool = True
    load_in_4bit: bool = True  # Enable 4-bit quantization for 8GB GPU
    max_new_tokens: int = 2048
    temperature: float = 0.7
    top_p: float = 0.9
    do_sample: bool = True  # Enable sampling for variation


@dataclass
class LayoutBlueprint:
    """Dynamic layout blueprint generated by LLM"""
    layout_id: str
    slide1_layout: Dict[str, Any]
    slide2_layout: Dict[str, Any]
    slide3_layout: Dict[str, Any]
    color_scheme: Dict[str, Tuple[int, int, int]]
    typography_scale: Dict[str, int]
    grid_config: Dict[str, Any]
    variation_seed: int
    generated_at: str = field(default_factory=lambda: datetime.now().isoformat())


class Qwen3VLEngine:
    """
    GPU-optimized Qwen3-VL engine for vision-language tasks.
    Handles image understanding and generative layout creation.
    """
    
    def __init__(self, config: VLMConfig = None):
        self.config = config or VLMConfig()
        self.model = None
        self.processor = None
        self._initialized = False
        self._device = torch.device(self.config.device if torch.cuda.is_available() else "cpu")
        
    def initialize(self) -> bool:
        """Initialize the model with GPU optimizations"""
        if self._initialized:
            return True
            
        try:
            print(f"ðŸš€ Initializing Qwen3-VL on {self._device}...")
            print(f"   GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}")
            print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB" if torch.cuda.is_available() else "")
            
            from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig
            
            # 4-bit quantization config for 8GB GPU
            if self.config.load_in_4bit:
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.bfloat16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
            else:
                quantization_config = None
            
            # Load model with optimizations
            self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                self.config.model_name,
                torch_dtype=getattr(torch, self.config.torch_dtype),
                quantization_config=quantization_config,
                device_map="auto",
                attn_implementation="flash_attention_2" if self.config.use_flash_attention else "eager",
                trust_remote_code=True
            )
            
            self.processor = AutoProcessor.from_pretrained(
                self.config.model_name,
                trust_remote_code=True
            )
            
            # Enable torch.compile for faster inference (PyTorch 2.0+)
            if hasattr(torch, 'compile') and torch.cuda.is_available():
                try:
                    self.model = torch.compile(self.model, mode="reduce-overhead")
                    print("   âœ“ torch.compile enabled")
                except Exception as e:
                    print(f"   âš  torch.compile skipped: {e}")
            
            self._initialized = True
            print(f"   âœ“ Model loaded successfully")
            
            # Clear cache
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            return True
            
        except ImportError as e:
            print(f"âš  VL model dependencies not available: {e}")
            print("   Falling back to text-only Ollama mode")
            return False
        except Exception as e:
            print(f"âš  Failed to initialize VL model: {e}")
            return False
    
    def _encode_image(self, image_path: str) -> Optional[str]:
        """Encode image to base64"""
        try:
            with open(image_path, 'rb') as f:
                return base64.b64encode(f.read()).decode('utf-8')
        except Exception as e:
            print(f"Error encoding image: {e}")
            return None
    
    def analyze_image(self, image_path: str, prompt: str) -> str:
        """Analyze an image using Qwen3-VL"""
        if not self._initialized:
            if not self.initialize():
                return ""
        
        try:
            from qwen_vl_utils import process_vision_info
            
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": f"file://{image_path}"},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
            
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
            image_inputs, video_inputs = process_vision_info(messages)
            
            inputs = self.processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt"
            ).to(self._device)
            
            with torch.inference_mode():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=self.config.max_new_tokens,
                    temperature=self.config.temperature,
                    top_p=self.config.top_p,
                    do_sample=self.config.do_sample
                )
            
            generated_ids = outputs[:, inputs.input_ids.shape[1]:]
            response = self.processor.batch_decode(
                generated_ids, skip_special_tokens=True
            )[0]
            
            return response
            
        except Exception as e:
            print(f"Error analyzing image: {e}")
            return ""
    
    def generate_layout_blueprint(self, company_data: Dict, sector: str, 
                                   variation_seed: int = None) -> LayoutBlueprint:
        """
        Generate a unique layout blueprint for each company.
        Uses LLM creativity + controlled randomness for variation.
        """
        if variation_seed is None:
            variation_seed = random.randint(1, 1000000)
        
        random.seed(variation_seed)
        
        # Generate unique layout ID
        layout_id = hashlib.md5(
            f"{company_data.get('name', 'unknown')}_{variation_seed}_{datetime.now().isoformat()}".encode()
        ).hexdigest()[:12]
        
        # Define layout variation options
        grid_options = [
            {"type": "2x2", "columns": 2, "rows": 2},
            {"type": "3x2", "columns": 3, "rows": 2},
            {"type": "1+3", "columns": 4, "main_span": 2},  # 1 large + 3 small
            {"type": "asymmetric", "columns": [0.4, 0.6]},
            {"type": "dashboard", "columns": 3, "metrics_top": True},
            {"type": "magazine", "columns": 2, "image_heavy": True},
        ]
        
        chart_style_options = [
            {"primary": "column_clustered", "secondary": "pie"},
            {"primary": "bar_clustered", "secondary": "doughnut"},
            {"primary": "line", "secondary": "pie"},
            {"primary": "area", "secondary": "bar_clustered"},
            {"primary": "column_stacked", "secondary": "pie"},
        ]
        
        # Color variations within Kelp branding
        accent_variations = [
            {"primary_accent": (0, 188, 212), "secondary_accent": (255, 105, 135)},  # Cyan + Pink
            {"primary_accent": (255, 105, 135), "secondary_accent": (255, 165, 90)},  # Pink + Orange
            {"primary_accent": (255, 165, 90), "secondary_accent": (0, 188, 212)},   # Orange + Cyan
            {"primary_accent": (0, 188, 212), "secondary_accent": (255, 165, 90)},   # Cyan + Orange
            {"primary_accent": (100, 80, 160), "secondary_accent": (0, 188, 212)},   # Purple + Cyan
        ]
        
        # Section arrangement options for each slide
        slide1_arrangements = [
            {"style": "classic", "desc_position": "top", "sections": "grid"},
            {"style": "modern", "desc_position": "left", "sections": "stack"},
            {"style": "bold", "desc_position": "center", "sections": "cards"},
            {"style": "minimal", "desc_position": "top", "sections": "list"},
            {"style": "dynamic", "desc_position": "floating", "sections": "asymmetric"},
        ]
        
        slide2_arrangements = [
            {"style": "dashboard", "charts_position": "center", "metrics": "top_cards"},
            {"style": "split", "charts_position": "left", "metrics": "right_panel"},
            {"style": "stacked", "charts_position": "bottom", "metrics": "top_grid"},
            {"style": "focused", "charts_position": "large_center", "metrics": "corners"},
            {"style": "infographic", "charts_position": "scattered", "metrics": "inline"},
        ]
        
        slide3_arrangements = [
            {"style": "numbered", "highlights_format": "numbered_cards"},
            {"style": "icons", "highlights_format": "icon_boxes"},
            {"style": "timeline", "highlights_format": "horizontal_flow"},
            {"style": "pillars", "highlights_format": "vertical_columns"},
            {"style": "spotlight", "highlights_format": "one_main_three_sub"},
        ]
        
        # Typography scale variations
        typography_scales = [
            {"title": 24, "subtitle": 16, "body": 11, "metric": 28, "caption": 9},
            {"title": 22, "subtitle": 14, "body": 10, "metric": 32, "caption": 8},
            {"title": 26, "subtitle": 18, "body": 12, "metric": 24, "caption": 10},
            {"title": 20, "subtitle": 15, "body": 11, "metric": 30, "caption": 9},
        ]
        
        # Box styling variations
        box_styles = [
            {"corner_radius": 0.15, "border_width": 1, "shadow": False},
            {"corner_radius": 0.25, "border_width": 0, "shadow": True},
            {"corner_radius": 0.1, "border_width": 2, "shadow": False},
            {"corner_radius": 0.3, "border_width": 1, "shadow": True},
            {"corner_radius": 0, "border_width": 1, "shadow": False},  # Sharp corners
        ]
        
        # Make random selections
        selected_grid = random.choice(grid_options)
        selected_chart_style = random.choice(chart_style_options)
        selected_accents = random.choice(accent_variations)
        selected_typography = random.choice(typography_scales)
        selected_box_style = random.choice(box_styles)
        
        # Generate unique margin/spacing variations
        margin_variation = {
            "left": 0.4 + random.uniform(0, 0.2),
            "right": 0.4 + random.uniform(0, 0.2),
            "top": 0.9 + random.uniform(0, 0.2),
            "bottom": 0.7 + random.uniform(0, 0.15),
            "gutter": 0.15 + random.uniform(0, 0.1)
        }
        
        blueprint = LayoutBlueprint(
            layout_id=layout_id,
            slide1_layout={
                "arrangement": random.choice(slide1_arrangements),
                "grid": selected_grid,
                "box_style": selected_box_style,
                "margins": margin_variation,
                "section_count": random.randint(3, 5),
                "show_description": True,
                "description_lines": random.randint(2, 4),
            },
            slide2_layout={
                "arrangement": random.choice(slide2_arrangements),
                "chart_styles": selected_chart_style,
                "metric_cards_count": random.randint(3, 5),
                "chart_count": random.randint(1, 2),
                "margins": margin_variation,
                "box_style": selected_box_style,
            },
            slide3_layout={
                "arrangement": random.choice(slide3_arrangements),
                "highlights_count": random.randint(3, 5),
                "show_future_outlook": random.choice([True, True, False]),
                "show_awards": random.choice([True, False]),
                "margins": margin_variation,
                "box_style": selected_box_style,
            },
            color_scheme={
                "primary": (45, 35, 75),  # Fixed Kelp primary
                "primary_light": (75, 55, 120),
                "accent1": selected_accents["primary_accent"],
                "accent2": selected_accents["secondary_accent"],
                "background": (255, 255, 255),
                "text_dark": (60, 60, 60),
                "text_light": (180, 180, 180),
            },
            typography_scale=selected_typography,
            grid_config=selected_grid,
            variation_seed=variation_seed
        )
        
        return blueprint
    
    def extract_web_insights(self, screenshot_path: str, url: str) -> Dict[str, Any]:
        """
        Use VL model to extract insights from web page screenshots.
        Understands charts, infographics, and visual data.
        """
        prompt = f"""Analyze this webpage screenshot from {url}.

Extract the following business intelligence:
1. Key metrics or numbers visible (revenue, growth, counts)
2. Product/service information
3. Company achievements or awards mentioned
4. Market positioning statements
5. Any charts or infographics - describe what they show

Format as JSON with keys: metrics, products, achievements, positioning, visual_data"""

        response = self.analyze_image(screenshot_path, prompt)
        
        try:
            # Try to parse as JSON
            if "```json" in response:
                response = response.split("```json")[1].split("```")[0]
            return json.loads(response)
        except:
            # Return raw insights
            return {"raw_insights": response}
    
    def generate_creative_content(self, prompt: str, context: Dict = None) -> str:
        """
        Generate creative text content with high variation.
        Used for unique anonymized descriptions, hooks, etc.
        """
        if not self._initialized:
            # Fallback to Ollama
            return self._ollama_generate(prompt, context)
        
        try:
            messages = [{"role": "user", "content": [{"type": "text", "text": prompt}]}]
            
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
            inputs = self.processor(
                text=[text],
                padding=True,
                return_tensors="pt"
            ).to(self._device)
            
            with torch.inference_mode():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=self.config.max_new_tokens,
                    temperature=0.8,  # Higher for creativity
                    top_p=0.95,
                    do_sample=True,
                    top_k=50
                )
            
            generated_ids = outputs[:, inputs.input_ids.shape[1]:]
            response = self.processor.batch_decode(
                generated_ids, skip_special_tokens=True
            )[0]
            
            return response
            
        except Exception as e:
            print(f"VL generation error: {e}, falling back to Ollama")
            return self._ollama_generate(prompt, context)
    
    def _ollama_generate(self, prompt: str, context: Dict = None) -> str:
        """Fallback to Ollama for text generation"""
        try:
            import requests
            response = requests.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": "qwen2.5:7b",
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.8,
                        "num_predict": 1024
                    }
                },
                timeout=120
            )
            if response.status_code == 200:
                return response.json().get('response', '')
        except:
            pass
        return ""
    
    def cleanup(self):
        """Release GPU memory"""
        if self.model is not None:
            del self.model
            self.model = None
        if self.processor is not None:
            del self.processor
            self.processor = None
        
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        self._initialized = False


# Global instance for reuse
_vl_engine: Optional[Qwen3VLEngine] = None


def get_vl_engine() -> Qwen3VLEngine:
    """Get or create the global VL engine instance"""
    global _vl_engine
    if _vl_engine is None:
        _vl_engine = Qwen3VLEngine()
    return _vl_engine


if __name__ == "__main__":
    print("Testing Qwen3-VL Engine...")
    
    engine = get_vl_engine()
    
    # Test layout blueprint generation
    test_data = {"name": "Test Company", "sector": "Manufacturing"}
    
    print("\nGenerating 3 different layout blueprints for same company:")
    for i in range(3):
        blueprint = engine.generate_layout_blueprint(test_data, "manufacturing")
        print(f"\nBlueprint {i+1} (seed: {blueprint.variation_seed}):")
        print(f"  Layout ID: {blueprint.layout_id}")
        print(f"  Slide 1: {blueprint.slide1_layout['arrangement']['style']}")
        print(f"  Slide 2: {blueprint.slide2_layout['arrangement']['style']}")
        print(f"  Slide 3: {blueprint.slide3_layout['arrangement']['style']}")
        print(f"  Accents: {blueprint.color_scheme['accent1']}, {blueprint.color_scheme['accent2']}")
